{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgex9ctIGdxMBZYU0C47tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pUrGe12/MLBxG-extension/blob/main/similaritySearchMLB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rlAxyXyvAkfk"
      },
      "outputs": [],
      "source": [
        "# !pip install pinecone\n",
        "# !pip install \"pinecone[grpc]\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import time\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global pinecone_api_key\n",
        "pinecone_api_key = \"\"\n"
      ],
      "metadata": {
        "id": "LsbnJZVXA2JF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_autoencoder(input_dim, encoding_dim=8):\n",
        "    \"\"\"Create an enhanced autoencoder with additional layers and features\"\"\"\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    x = BatchNormalization()(input_layer)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    encoded = Dense(encoding_dim, activation='relu', name='encoder')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Dense(16, activation='relu')(encoded)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    decoded = Dense(input_dim, activation='linear')(x)\n",
        "\n",
        "    # Create models\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    encoder = Model(input_layer, encoded)\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "\n",
        "def prepare_and_train_autoencoder(data, encoding_dim=8):\n",
        "    \"\"\"Prepare data and train the autoencoder\"\"\"\n",
        "    # Initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "    normalized_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Create model\n",
        "    input_dim = normalized_data.shape[1]\n",
        "    autoencoder, encoder = create_enhanced_autoencoder(input_dim, encoding_dim)\n",
        "\n",
        "    # Compile\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = autoencoder.fit(\n",
        "        normalized_data,\n",
        "        normalized_data,\n",
        "        epochs=200,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return encoder, scaler, history\n"
      ],
      "metadata": {
        "id": "peZzyAxdBJGv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_pinecone(embeddings, df, pinecone_api_key, index_name=\"baseball-hits\"):\n",
        "    \"\"\"Save embeddings to Pinecone\"\"\"\n",
        "    pinecone = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "    if not pinecone.has_index(index_name):\n",
        "        pinecone.create_index(name=index_name,\n",
        "                              dimension=embeddings.shape[1],\n",
        "                              spec=ServerlessSpec(\n",
        "                                  cloud='aws',\n",
        "                                  region='us-east-1')\n",
        "                              )\n",
        "\n",
        "    # Wait for the index to be ready\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "    index = pinecone.Index(index_name)\n",
        "\n",
        "    vectors = []\n",
        "    for i, (embedding, row) in enumerate(zip(embeddings, df.iterrows())):\n",
        "        vectors.append({\n",
        "            'id': str(row[1]['play_id']),\n",
        "            'values': embedding.tolist(),\n",
        "            'metadata': {\n",
        "                'title': row[1]['title'],\n",
        "                'exit_velocity': float(row[1]['ExitVelocity']),\n",
        "                'hit_distance': float(row[1]['HitDistance']),\n",
        "                'launch_angle': float(row[1]['LaunchAngle'])\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Upload in batches\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(vectors), batch_size):\n",
        "        batch = vectors[i:i + batch_size]\n",
        "        index.upsert(vectors=batch)"
      ],
      "metadata": {
        "id": "N_Z7Oe6SBQL5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_new_hit(new_hit_data, encoder, scaler):\n",
        "    \"\"\"Process a new hit and generate its embedding\"\"\"\n",
        "    features = np.array([[\n",
        "        new_hit_data['ExitVelocity'],\n",
        "        new_hit_data['HitDistance'],\n",
        "        new_hit_data['LaunchAngle']\n",
        "    ]])\n",
        "\n",
        "    embedding = encoder.predict(scaler.transform(features))\n",
        "    return embedding[0]\n",
        "\n",
        "def find_similar_hits(embedding, index_name=\"baseball-hits\", top_k=5):\n",
        "    \"\"\"Find similar hits in the database\"\"\"\n",
        "    pinecone = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "    index = pinecone.Index(index_name)\n",
        "    results = index.query(\n",
        "        vector=embedding.tolist(),\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return results\n",
        "\n",
        "def print_similar_hits(results):\n",
        "    \"\"\"Pretty print the similar hits\"\"\"\n",
        "    print(\"\\nMost Similar Hits Found:\")\n",
        "    print(\"-\" * 80)\n",
        "    for idx, match in enumerate(results.matches, 1):\n",
        "        print(f\"\\n{idx}. Similarity Score: {match.score:.3f}\")\n",
        "        print(f\"Title: {match.metadata['title']}\")\n",
        "        print(f\"Exit Velocity: {match.metadata['exit_velocity']:.1f} mph\")\n",
        "        print(f\"Hit Distance: {match.metadata['hit_distance']:.1f} feet\")\n",
        "        print(f\"Launch Angle: {match.metadata['launch_angle']:.1f} degrees\")"
      ],
      "metadata": {
        "id": "AgQrPSyXBYNg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_training(df, pinecone_api_key):\n",
        "    \"\"\"Initial training and setup\"\"\"\n",
        "\n",
        "    features = df[['ExitVelocity', 'HitDistance', 'LaunchAngle']].values\n",
        "\n",
        "    # Train models\n",
        "    encoder, scaler, history = prepare_and_train_autoencoder(features)\n",
        "\n",
        "    # Generate embeddings for all data\n",
        "    embeddings = encoder.predict(scaler.transform(features))\n",
        "\n",
        "    # Save models\n",
        "    if not os.path.exists('models'):\n",
        "        os.makedirs('models')\n",
        "    save_model(encoder, 'models/encoder_model.h5')\n",
        "    joblib.dump(scaler, 'models/scaler.joblib')\n",
        "\n",
        "    save_to_pinecone(embeddings, df, pinecone_api_key)\n",
        "\n",
        "    return encoder, scaler\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"Load saved models\"\"\"\n",
        "    encoder = load_model('models/encoder_model.h5')\n",
        "    scaler = joblib.load('models/scaler.joblib')\n",
        "    return encoder, scaler"
      ],
      "metadata": {
        "id": "jcMWrfACBlJR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train\n",
        "\n",
        "df1 = pd.read_csv(\"2016-mlb-homeruns.csv\")\n",
        "df2 = pd.read_csv(\"2017-mlb-homeruns.csv\")\n",
        "df3 = pd.read_csv(\"2024-mlb-homeruns.csv\")\n",
        "\n",
        "df1 = df1.dropna(subset=[\"title\"])\n",
        "df2 = df2.dropna(subset=[\"title\"])\n",
        "df3 = df3.dropna(subset=[\"title\"]) # Remove the title if it is a null value\n",
        "\n",
        "numeric_cols = df1.select_dtypes(include=np.number).columns\n",
        "df1[numeric_cols] = df1[numeric_cols].fillna(df1[numeric_cols].mean())\n",
        "\n",
        "numeric_cols2 = df1.select_dtypes(include=np.number).columns\n",
        "df2[numeric_cols2] = df2[numeric_cols2].fillna(df2[numeric_cols2].mean())\n",
        "\n",
        "numeric_cols3 = df3.select_dtypes(include=np.number).columns\n",
        "df3[numeric_cols3] = df3[numeric_cols3].fillna(df3[numeric_cols3].mean())\n",
        "\n",
        "# Prepare features\n",
        "df = pd.concat([df1, df2, df3], ignore_index = True)\n",
        "\n",
        "encoder, scaler = initial_training(df, pinecone_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy3UBaAcBr88",
        "outputId": "ef79e774-29af-4185-f9d8-2f20508ea212"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.5486 - val_loss: 0.0262\n",
            "Epoch 2/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1019 - val_loss: 0.0214\n",
            "Epoch 3/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0918 - val_loss: 0.0250\n",
            "Epoch 4/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0898 - val_loss: 0.0200\n",
            "Epoch 5/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0839 - val_loss: 0.0166\n",
            "Epoch 6/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0927 - val_loss: 0.0173\n",
            "Epoch 7/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0789 - val_loss: 0.0220\n",
            "Epoch 8/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0796 - val_loss: 0.0246\n",
            "Epoch 9/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0854 - val_loss: 0.0140\n",
            "Epoch 10/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0862 - val_loss: 0.0178\n",
            "Epoch 11/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0800 - val_loss: 0.0166\n",
            "Epoch 12/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0744 - val_loss: 0.0210\n",
            "Epoch 13/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0733 - val_loss: 0.0169\n",
            "Epoch 14/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0787 - val_loss: 0.0201\n",
            "Epoch 15/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0754 - val_loss: 0.0189\n",
            "Epoch 16/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0785 - val_loss: 0.0152\n",
            "Epoch 17/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0744 - val_loss: 0.0164\n",
            "Epoch 18/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0758 - val_loss: 0.0159\n",
            "Epoch 19/200\n",
            "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0680 - val_loss: 0.0142\n",
            "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder, scaler = load_models()\n",
        "\n",
        "# Process a new hit\n",
        "new_hit = {\n",
        "    'ExitVelocity': 100.5,\n",
        "    'HitDistance': 402.0,\n",
        "    'LaunchAngle': 31.0\n",
        "}\n",
        "\n",
        "# Generate embedding and find similar hits\n",
        "embedding = process_new_hit(new_hit, encoder, scaler)\n",
        "similar_hits = find_similar_hits(embedding, top_k=5)\n",
        "print_similar_hits(similar_hits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llsTs_MVCGfh",
        "outputId": "8a29e14a-3d41-451d-9fff-31739e881c3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\n",
            "Most Similar Hits Found:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Similarity Score: 1.000\n",
            "Title: Francisco Lindor homers (14) on a fly ball to left field.\n",
            "Exit Velocity: 100.5 mph\n",
            "Hit Distance: 403.1 feet\n",
            "Launch Angle: 31.0 degrees\n",
            "\n",
            "2. Similarity Score: 1.000\n",
            "Title: Khris Davis homers (12) on a fly ball to center field.\n",
            "Exit Velocity: 100.5 mph\n",
            "Hit Distance: 404.0 feet\n",
            "Launch Angle: 31.0 degrees\n",
            "\n",
            "3. Similarity Score: 1.000\n",
            "Title: Rickie Weeks Jr.  homers (6) on a fly ball to left center field.   Paul Goldschmidt scores.\n",
            "Exit Velocity: 100.6 mph\n",
            "Hit Distance: 402.0 feet\n",
            "Launch Angle: 31.0 degrees\n",
            "\n",
            "4. Similarity Score: 1.000\n",
            "Title: Eduardo Escobar homers (3) on a fly ball to center field.   Jorge Polanco scores.\n",
            "Exit Velocity: 100.6 mph\n",
            "Hit Distance: 398.0 feet\n",
            "Launch Angle: 31.0 degrees\n",
            "\n",
            "5. Similarity Score: 1.000\n",
            "Title: Lars Nootbaar homers (12) on a fly ball to center field.\n",
            "Exit Velocity: 100.7 mph\n",
            "Hit Distance: 400.1 feet\n",
            "Launch Angle: 31.0 degrees\n"
          ]
        }
      ]
    }
  ]
}
